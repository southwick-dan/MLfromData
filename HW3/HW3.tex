\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amssymb,mathtools,xparse,graphicx,float,datetime,color,array,graphics,enumerate,tikz,pgfplots,xcolor}
\usepgfplotslibrary{statistics}
\pagestyle{empty}
\newcommand{\D}{\displaystyle}
\setlength{\textheight}{9in} \setlength{\headheight}{.2in}
\setlength{\headsep}{0in} \setlength{\topmargin}{0in}
\begin{document}
\begin{center}
CSCI 6100 Machine Learning From Data\\
Fall 2018\\
\end{center}
\begin{center}
HOMEWORK 3\\
Daniel Southwick\\
661542908\\
southd@rpi.edu
\end{center}
\vspace{.1in}

\noindent {\bf Exercise 1.13} \\\\
\indent (a)There will be two cases when $h(x)$ makes an error approximating $f(x)$. One is when $y = f(x)$ and $h(x)$ approximates correctly and one is when $y \neq f(x)$ and $h(x)$ approximates incorrectly. In both cases $h(x)$ will approximates $f(x)$. So the error it makes: \\ \indent $P[error] = \mu*\lambda + (1-\mu)*(1-\lambda)$\\\\
\indent (b)When the performance of $h(x)$ is independent of $\mu$, we know that $P[error]$ must be independent of $\mu$ \\\indent$P[error] = \mu*\lambda + (1-\mu)*(1-\lambda) = 2*\mu*\lambda - \mu - \lambda +1$\\ \indent So $2*\mu*\lambda - \mu = 0$ Thus $\mu = 0.5$\\

\noindent {\bf Exercise 2.1} \\\\
\indent(a)Positive Rays: $m_H(N) = N+1$. $m_H(1) = 1+1 = 2$, $m_H(2) = N+2 = 3 < 2^2 = 4$, so 2 is a breakpoint for $H$\\\\
\indent(b)Positive Intervals: $m_H(N) = \frac{1}{2} N^2+\frac{1}{2} N+1$, $m_H(2) = \frac{1}{2}\times2^2+\frac{1}{2}\times2+1 = 4 = 4$, $m_H(3) = \frac{1}{2}\times3^2+\frac{1}{2}\times3+1 = 4 = 7 < 2^3 = 8$, so 3 is a breakpoint for $H$\\\\
\indent(c)Convex Set:  $m_H(N) = 2^N = 2^N$. There's no breakpoints since $m_H(N) = 2^N$\\

\noindent {\bf Exercise 2.2} \\\\
\indent (a) \\
\indent (i)Positive Rays: From Exercise 2.1, $m_H(N) = N+1$ with break point $k = 2$, 
\begin{center}
$m_H(N) = 2+1 = 3 \leq {N\choose 0} + {N\choose 1} = N+1 = 3$\\
\end{center}
Thus the theorem holds\\\\
\indent (ii)Positive Intervals: From Exercise 2.1, $m_H(N) =  \frac{1}{2} N^2+\frac{1}{2} N+1 $ with break point $k = 3$, 
\begin{center}
$m_H(N) =  \frac{1}{2} N^2+\frac{1}{2} N+1 = 7 \leq {N\choose 0} + {N\choose 1} + {N\choose 2}=  \frac{1}{2} N^2+\frac{1}{2} N+1 = 7$\\
\end{center}
Thus the theorem holds\\\\
\indent (iii)Convex Sets: From Exercise 2.1, $m_H(N) =  2^N$ with no break point. Thus no bound exists.\\\\
\indent (b) $m_H(N) = N+2^{\left \lfloor{N/2}\right \rfloor}$. There exists breakpoints $k$ such that $m_H(k) < 2^k$. But from the theorem, for $N \leq k$, we can find a polynomial bound on $m_H(N)$ so $\lim_{x\to\infty} \frac{1}{N}log m_H(N) = 0$, but for actual $m_H(N), \lim_{x\to\infty} \frac{1}{N}log m_H(N) = \frac{1}{2} log 2$, which is inconsistence with the hypothesis. Thus, $m_H(N)$ cannot be bounded by any polynomial and there does not exist any hypothesis set. \\

\noindent {\bf Exercise 2.3} \\\\
\indent(a)Positive Rays: From Exercise 2.1, Smallest Breakpoint is 2, then $d_{vc} = 2 - 1 = 1$\\\\
\indent(b)Positive Intervals: From Exercise 2.1, Smallest Breakpoint is 3, then $d_{vc} = 3 - 1 = 2$\\\\
\indent(c)Convex Sets: From Exercise 2.1, Smallest Breakpoint is 2, then $d_{vc} = \infty - 1 = \infty$\\\\

\noindent {\bf Exercise 2.6} \\\\
\indent (a)$\displaystyle E_{out}(g) \leq E_{in}(g)+\sqrt{\frac{1}{2N}\ln\frac{2M}{\delta}}$\\
For the testing error ($E_{out}$): \\$M = 1, N = 200, \delta = 0.05$, so error bound $\displaystyle \sqrt{\frac{1}{2\times 200}\ln\frac{2\times 1}{\delta}} = 0.096$\\\\
For the training error ($E_{in}$): \\$M = 1000, N = 400, \delta = 0.05$, so error bound $\displaystyle \sqrt{\frac{1}{2\times 400}\ln\frac{2\times 1000}{\delta}} = 0.115$\\
Thus $E_{in}$ has a higher error bar.\\\\
\indent (b) If we reserve more examples for testing, we can find a better fitted g, but then there are also less examples as training set, then the testing of g could be come useless with a large $E_{test}(g)$ and $E_{out}$ could not be approximate to 0 since we did not find a good g to begin with.\\

\noindent {\bf Problem 1.11} \\\\
\indent (a) \begin{center} \begin{tabular}{cc  cccc cc}
		 &$f$&&&&&&$f$\\
		$h$&\begin{tabular}{c |  c c}
			&$+1$&$-1$\\\hline
			$+1$&$0$&$1$\\
			$-1$&$10$&$0$
		\end{tabular} 
		&&&&&$h$&\begin{tabular}{c | c c}
			&$+1$&$-1$\\\hline
			$+1$&$0$&$1000$\\
			$-1$&$1$&$0$
		\end{tabular} \\
		&Supermarket&&&&&&CIA\\
		\end{tabular}
		\end{center} 
\newpage
\begin{center}
For the CIA application: the in-sample error: 
$\displaystyle E_{in} = \frac{1}{N}\sum_{i=1}^{N} (1*C_{[h(x_i)=-1,f(x_i)=+1]} + 1000*C_{[h(x_i)=+1,f(x_i)=-1]})$\\
($C_{[n]} = 1$ if $n$ is true, else 0.)
\end{center}

\begin{center}
For the Supermarket application: the in-sample error: 
$\displaystyle E_{in} = \frac{1}{N}\sum_{i=1}^{N} (10*C_{[h(x_i)=-1,f(x_i)=+1]} + 1*C_{[h(x_i)=+1,f(x_i)=-1]})$\\
($C_{[n]} = 1$ if $n$ is true, else 0.)
\end{center}

\noindent {\bf Problem 1.12} \\\\
\indent(a) \begin{center} 
$\displaystyle E_{in}(h) = \sum_{i = 1}^{N}(h-y_n)^2$\\$\displaystyle = Nh^2 - 2h\sum_{i = 1}^{N}y_n + \sum_{i=1}^{N}(y_n)^2$\\$\displaystyle=N(h-\frac{1}{N}\sum_{i=1}^{N}y_n)^2 + \sum_{i=1}^{N}y_n^2 - \frac{1}{N}(\sum_{i=1}^{N}y_n)^2$ 
\end{center}
Since both $\displaystyle \sum_{i=1}^{N}y_n^2$ and $\displaystyle \frac{1}{N}(\sum_{i=1}^{N}y_n)^2$ terms are constant, so $min(E_{in})$ reach its minimum when $\displaystyle h = \frac{1}{N}\sum_{i=1}^{N}y_n$.\\\\\\
\indent(b) 
\begin{center}
$\displaystyle E_{in}(h) = \sum_{n=1}^{N}|h-y_n|$
\end{center}
\indent We know that N data points are in the order of $y_1 \leq y_2 \leq y_3 \leq ... \leq y_N$, we suppose $h$ lies between $y_k$ and $y_{k+1}$. Now, if $N > 2k$, $y_1, y_2, ... , y_k, h, y_{k+1}, y_{k+2}, ... , y_{N-k+1}, ... , y_N$, then: \\
\begin{center}
$\displaystyle E_{in}(h) = \sum_{i = 1}^{k}(y_{N-i+1} - y_n) + \sum_{i = k+1}^{N-k} |y_i - h|$
\end{center}
If $N < 2k$, $y_1, y_2, ... , y_{N-k}, y_{N-k+1} ... , y_k, h, y_{k+1}, ... , y_N$, then: \\
\begin{center}
$\displaystyle E_{in}(h) = \sum_{i = 1}^{N-k}(y_{N-i+1} - y_n) + \sum_{i = N-k+1}^{k} |y_i - h|$
\end{center}
If $h$ moves to the left direction, then the first term decrease and the second term stays the same. So when $E_{in}(h) achieves its minimum, $h$ moves toward the middle position$ So if $N$ is odd, $h$ equals to the median of the $N$ data points, $h = y_{\frac{N+1}{2}}$; If $N$ is even, $h$ is between $y_{\frac{N}{2}}$ and $y_{\frac{N}{2}+1}$. Either case, half the data points are at most $h_{med}$ and half the data points are at least $h_{med}$, thus the estimate will be in $h_{med}$\\\\
\indent(c)
If $y_N$ is perturbed to $y+\epsilon$, where $\epsilon\rightarrow\infty$, $h_{mean}$ will grow to unbounded. 
However, $h_{med}$ will stay the same. $h_{med}$ will always lie between $y_{N/2}$ and $y_{N/2+1}$ when $N$ is even, or equal to $y_{N/2+1}$ when $N$ is odd.
\end{document}
